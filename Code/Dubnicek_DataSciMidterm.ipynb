{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm exam\n",
    "\n",
    "This exam is open-notebook and open-web. It's also open-instructor, in the coding sections of the exam; if you run into a bug that you can't explain after a few tries, I'm willing to take a look at your code.\n",
    "\n",
    "We start with three short essay questions, each of which can be answered in a paragraph or so. The first two paragraphs can be pretty brief. The third one might run a bit longer, or might become two paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short essay questions.\n",
    "\n",
    "#### 1. Overfitting.\n",
    "\n",
    "What is \"overfitting\"? And why is this problem especially likely to arise when we model unstructured datasets (for instance, a collection of tweets or novels, rather than a simple table with five or six columns)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for 1:\n",
    "\n",
    "Overfitting describes the problem that occurs when training a predictive model against the data you wish to build a predictive model for. Especially this is in encountered when using the entirety of a dataset to train the model you wish to use on that same dataset. With unstructured data, you encounter more issues with overfitting due to having more interpretation fall on the back of the algorithm instead of on the data, due to a lack of data constraints. For example, a table of rain fall numbers from countries across the US has both fewer pieces of data, when compared to a novel in a text file, and a uniform structure of data--e.g. a city name, a rain amount, and a date. With this uniform data structure, the algorithm is doing less interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cross-validation.\n",
    "\n",
    "What does it mean to cross-validate a model? What's the point of doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for 2:\n",
    "\n",
    "Cross-validation is the practice of using a portion of a dataset that you wish to test to train the predictive model, and to rotate the portion used for training and test the results. For example, if you have 100 batting averages for 100 players in the MLB, you'd use 25 players to train a model to predict batting average based on number of letters in a player's name (ridiculous, I know!), and then actually run the model on the remaining 75. Cross-validation would be rotating which 25 players are the training corpus and which 75 players are the test corpus. This way you, ideally, prevent the predictive model from not being a predictive model at all, but being overfit to your dataset, and merely an interpretive tool for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Interpreting models of the human past.\n",
    "\n",
    "Beyond general quantitative pitfalls like \"overfitting,\" why does it become challenging to draw historical and cultural conclusions from a quantitative model?\n",
    "\n",
    "Choose any (single) article from the set we discussed in the week of March 12, and explain why the interpretive problems it confronts emerge specifically from the complexity of the human subject-matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for 3:\n",
    "\n",
    "There are a number of reasons for the challenge. As David Bamman describes, it is important to remember that human subjective judgment is at the core of all computation, at some point. This human interpretation--in this piece, Dr. Bamman is looking at NLP functions, such as part of speech tagging, named entity recognition, etc.--is itself very subjective and abstract. Do we consider \"the Olympics,\" for instance, a place, event or organization? Any question or decision tough for humans to answer will, necessarily, be tough for machines to answer, because it must be posed to a machine *by* a human. The fuzziness here is also apparent in that there is often no objective truth--many times things can be interpreted in two different ways to equal levels of validity. Edgar Allan Poe had both a wife and a cousin, and he was married to both, in the form of one person. While a simple analogy, this shows the issue with messy, abstract human data and more objective, concrete forms of computational analysis used to interpret, and pry some truth from, this messy data.\n",
    "\n",
    "With the abstraction in data very high, what becomes important is the purpose of the analysis, and the defined scale of interpretation it puts forward. Dr. Bamman showed an interesting exercise with judging positivity of Romeo and Juliet, crowdsourcing sentiment analysis from 5 different humans on Mechanical Turk. There are areas of high variability between human assessors, illustrating the abstraction and variability of something as nebulous as \"is this scene positive or negative on a scale of -5 to 5?\" This goes to show that there will likely be variability in the machine interpretation as well, but hopefully it falls within the composite trajectory of the 5 humans put together. For me, the goal in this type of computational analysis is not a lofty idea of \"real truth,\" whatever that might mean, and instead a goal of replication what a sample of humans might find to be true, in aggregate and on average. Because of this, it's not surprising that some will refute or take issue with conclusions from this type of analysis, as their interpretation might be very different. Similarly, what is true in the aggregate really has no bearing on what is true in one instance. Though it rains about half of the year in Dublin, Ireland, it doesn't mean, on any given day, it can't be sunny and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Surviving the *Titanic.*\n",
    "\n",
    "We have good records about the passengers who were aboard the *Titanic* when it sank in 1912. (The provenance of the data is difficult to track, but I think this particular form of the dataset comes from [a Kaggle machine learning competition.](https://www.kaggle.com/c/titanic))\n",
    "\n",
    "Let's use a sample of the data to rehearse methods of exploratory data analysis in Pandas.\n",
    "\n",
    "First, read in the dataset (```titanic.csv```). What do you have?\n",
    "\n",
    "The dataset has twelve columns, but here are some important or perplexing ones:\n",
    "\n",
    "    **Survived**: A value of 1 indicates that the passenger survived. 0, didn't.\n",
    "    **Pclass**: Did the passenger buy a 1st, 2nd, or 3rd-class ticket?\n",
    "    **Sex**: Is coded as \"male\" or \"female.\" This may really be \"gender,\" since I doubt that ticket agents checked the passengers' biological sex in 1912, but we'll let that pass.\n",
    "    **Age**: In years.\n",
    "    **Embarked**: Which port the passenger sailed from.\n",
    "    **Sibsp**: How many siblings or spouses the passenger had on board.\n",
    "    **Parch**: How many parents or children the passenger had on board.\n",
    "\n",
    "Let's start by answering some simple questions.\n",
    "\n",
    "#### 1. What percentage of passengers survived, overall?\n",
    "#### 2. What was the gender balance, overall, among passengers? Say, what fraction were women?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the dataset, and use the ```.head()``` method to glance at the first few rows.\n",
    "cwd = os.getcwd()\n",
    "# print('Current working directory: ' + cwd + '\\n')     \n",
    "relativepath = os.path.join('..', 'data', 'titanic.csv')\n",
    "# print(relativepath)\n",
    "titanic = pd.read_csv(relativepath)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger survivor total:  342\n",
      "Passenger death total:  549\n",
      "Percentage of survivors:  38.38 %\n",
      "Male passengers:  577\n",
      "Female passengers:  314\n",
      "Females make up  35.0 % of passengers overall\n",
      "Males make up  65.0 % of passengers overall\n"
     ]
    }
   ],
   "source": [
    "# Insert code here to calculate (1) the fraction of passengers who survived, and (2) the fraction who were women.\n",
    "# to calculate total number of passengers, I pull max for the PID column, which starts at 1 and is increased by \n",
    "# 1 for each passenger in the dataset:\n",
    "\n",
    "num_passengers = titanic['PassengerId'].max()\n",
    "num_passengers \n",
    "\n",
    "# titanic['Survived']\n",
    "\n",
    "survive_count = titanic['Survived'].sum()\n",
    "print('Passenger survivor total: ', survive_count)\n",
    "death_count = num_passengers - survive_count\n",
    "print('Passenger death total: ', death_count)\n",
    "\n",
    "percent_survived = survive_count/num_passengers\n",
    "print('Percentage of survivors: ', round((percent_survived*100),2), '%')\n",
    "\n",
    "male = 'male'\n",
    "female = 'female'\n",
    "\n",
    "male_pass = 0\n",
    "female_pass = 0\n",
    "male_survivors = 0\n",
    "female_survivors = 0\n",
    "\n",
    "# titanic.head(150)\n",
    "\n",
    "for psgr in titanic['Sex']:\n",
    "    # print(psgr)\n",
    "    if psgr == 'female':\n",
    "        female_pass += 1\n",
    "    else:\n",
    "        male_pass += 1\n",
    "\n",
    "percent_male = round((male_pass / num_passengers), 2)\n",
    "percent_female = round((female_pass / num_passengers), 2)\n",
    "        \n",
    "print('Male passengers: ', male_pass)\n",
    "print('Female passengers: ', female_pass)\n",
    "print('Females make up ', (100*percent_female), '% of passengers overall')\n",
    "print('Males make up ', (100*percent_male), '% of passengers overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Now let's make that slightly more complex. Did the passengers really follow a policy of \"women and children first\"?\n",
    "\n",
    "Let's start answer that by producing a bar graph that indicates how the probability of survival varied across Sex. In other words, we want to have one bar that indicates the probability of survival for men (not the raw number of men surviving but the *fraction* who survived), and a second bar indicating the probability for women.\n",
    "\n",
    "You can use the split-apply-combine method from week 3 to summarize the data. Note that simply averaging (taking the mean) across a column that is either 0 or 1 will in effect give you the probability of finding a 1 in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f23c6e49225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and then produce a bar graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mby_sex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mby_sex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic' is not defined"
     ]
    }
   ],
   "source": [
    "# (3): Code is needed here to split-apply-combine,\n",
    "# and then produce a bar graph.\n",
    "\n",
    "by_sex = titanic.groupby('Sex')\n",
    "by_sex.head()\n",
    "\n",
    "titanic_sex = by_sex.aggregate(np.sum)['Survived']\n",
    "type(titanic_sex)\n",
    "\n",
    "titanic_sex_df = pd.DataFrame(titanic_sex)\n",
    "titanic_sex_df.head()\n",
    "\n",
    "pass_sex_list = [314, 577] # passengers by sex, from above\n",
    "psex = pd.Series(pass_sex_list)\n",
    "titanic_sex_df['TotalPassengers'] = psex.values\n",
    "titanic_sex_df.head()\n",
    "\n",
    "# print(titanic_sex_df.columns)\n",
    "\n",
    "grouped_surv = titanic_sex_df.groupby('Survived')\n",
    "grouped_surv.head()\n",
    "\n",
    "# # remember: two separate plot commands given in succession will render on the same plot. So, could create two dataframes\n",
    "# # and give two plot calls, one for each, and this should work.\n",
    "\n",
    "grouped_surv.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. One final twist.\n",
    "\n",
    "What about the relationship of \"ticket class\" to survival? And how did that interact with gender?\n",
    "\n",
    "See if you can produce a visualization that plots probability of survival broken out by Pclass and Sex at the same time. For instance, it could take the form of two lines (one for men and one for women), with the y axis indicating probability of survival, and the x axis indicating ticket class (1, 2, or 3). It's also possible to achieve the same thing with a bar graph (using paired bars). We did a version of that in week 3. Either solution is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAE3CAYAAABRmAGSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1KDDHdJapDhLkkNMtwlqUH/H42GJlSNxXAwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113783908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## (4): Code is needed here to produce a visualization that\n",
    "## reveals probability of survival broken out by Pclass\n",
    "## and Sex at the same time.\n",
    "\n",
    "by_class = titanic.groupby(['Pclass','Sex']).aggregate(np.mean)['Survived']\n",
    "by_class.head()\n",
    "\n",
    "by_class.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Did this a bit different, as discussed, but the visualization should be the same, with an int before each label of \n",
    "#  male or female that indicates their class, and each bar indicating a probability of survival\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ham or spam?\n",
    "\n",
    "Now let's move to predictive modeling of less structured data.\n",
    "\n",
    "#### Part 1.\n",
    "\n",
    "I've provided you with part of [a spam dataset developed by Almeida et al.](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/) Each row in this dataset contains an SMS message. 747 of them were flagged by users as \"spam,\" and 747 are legitimate--for our purposes, \"ham.\" (In case this etymology is vanishing in the mists of time: spam was originally a kind of canned meat.)\n",
    "\n",
    "There are two columns in the dataset. **Category** contains a flag indicating whether the message is spam; **text** contains the raw text of the message itself.\n",
    "\n",
    "Your first goals are to:\n",
    "\n",
    "1. Read this dataset (```hamorspam.csv```) in as a pandas DataFrame, and create a new column ```isspam```, which contains 1 if the row is spam, and 0 otherwise.\n",
    "\n",
    "2. Create a termdoc matrix based on the top 1000 words in the dataset. Use the ```.head()``` method to print out a few rows.\n",
    "\n",
    "3. Import multinomial Naive Bayes amd cross_val_score from sklearn; then do five-fold crossvalidation to estimate the accuracy of multinomial Naive Bayes on this dataset.\n",
    "\n",
    "Try to do each of those things in a separate cell of this notebook. The homework solution from week 6 (Feb 19-25) should be a useful guide here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>You are being contacted by our dating service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Guess who am I?This is the first time I create...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>You have been selected to stay in 1 of 250 top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>I don't know u and u don't know me. Send CHAT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>How's it going? Got any exciting karaoke type ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah I can still give you a ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are a very very very very bad girl. Or lady.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are we doing the norm tomorrow? I finish just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Call FREEPHONE 0800 542 0578 now!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0     spam  You are being contacted by our dating service ...\n",
       "1     spam  Please call our customer service representativ...\n",
       "2     spam  Guess who am I?This is the first time I create...\n",
       "3     spam  You have been selected to stay in 1 of 250 top...\n",
       "4     spam  I don't know u and u don't know me. Send CHAT ...\n",
       "5      ham  How's it going? Got any exciting karaoke type ...\n",
       "6      ham                   Yeah I can still give you a ride\n",
       "7      ham   You are a very very very very bad girl. Or lady.\n",
       "8      ham  Are we doing the norm tomorrow? I finish just ...\n",
       "9     spam                  Call FREEPHONE 0800 542 0578 now!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1): Read in hamorspam.csv; create a numeric column \"isspam.\"\n",
    "cwd = os.getcwd()\n",
    "relativepath = os.path.join('..', 'data', 'hamorspam.csv')\n",
    "ham_spam = pd.read_csv(relativepath)\n",
    "ham_spam.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>You are being contacted by our dating service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Guess who am I?This is the first time I create...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>You have been selected to stay in 1 of 250 top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>I don't know u and u don't know me. Send CHAT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>How's it going? Got any exciting karaoke type ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah I can still give you a ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are a very very very very bad girl. Or lady.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are we doing the norm tomorrow? I finish just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Call FREEPHONE 0800 542 0578 now!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sat right? Okay thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ham</td>\n",
       "      <td>Can a not?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>Summers finally here! Fancy a chat or flirt wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spam</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are you up for the challenge? I know i am :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0      spam  You are being contacted by our dating service ...\n",
       "1      spam  Please call our customer service representativ...\n",
       "2      spam  Guess who am I?This is the first time I create...\n",
       "3      spam  You have been selected to stay in 1 of 250 top...\n",
       "4      spam  I don't know u and u don't know me. Send CHAT ...\n",
       "5       ham  How's it going? Got any exciting karaoke type ...\n",
       "6       ham                   Yeah I can still give you a ride\n",
       "7       ham   You are a very very very very bad girl. Or lady.\n",
       "8       ham  Are we doing the norm tomorrow? I finish just ...\n",
       "9      spam                  Call FREEPHONE 0800 542 0578 now!\n",
       "10      ham                         Sat right? Okay thanks... \n",
       "11      ham                                         Can a not?\n",
       "12     spam  Summers finally here! Fancy a chat or flirt wi...\n",
       "13     spam  Please call our customer service representativ...\n",
       "14      ham       Are you up for the challenge? I know i am :)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spam_test(a_data_frame, rowidx):    \n",
    "    if 'spam' in a_data_frame['category'][rowidx]:\n",
    "        return 'spam'\n",
    "    elif 'ham' in a_data_frame['category'][rowidx]:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "ham_or_spam = ham_spam['text']\n",
    "source = []\n",
    "for idx in ham_spam.index:\n",
    "    source.append(spam_test(ham_spam, idx))\n",
    "# print(source)\n",
    "\n",
    "ham_spam_num = []\n",
    "\n",
    "for item in ham_spam['category']:\n",
    "    if item == 'ham':\n",
    "        ham_spam_num.append(0)\n",
    "    else:\n",
    "        ham_spam_num.append(1)\n",
    "        \n",
    "# print(ham_spam_num)\n",
    "\n",
    "hs_series = pd.Series(ham_spam_num)\n",
    "# ham_spam['isspam'] = hs_series.values\n",
    "\n",
    "ham_spam.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1494x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20171 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2): Create a termdoc matrix. There are several ways to do this, but CountVectorizer\n",
    "# will save you some work.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(max_features = 1000)\n",
    "\n",
    "hs_sparse_matrix = countvec.fit_transform(ham_spam['text'])\n",
    "hs_sparse_matrix\n",
    "# hs_termdoc = pd.DataFrame(hs_sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "# hs_termdoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rdubnic2/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hs_termdoc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bc0ec3888341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mham_spam_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs_termdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mham_spam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isspam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_spam_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_spam_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mham_spam_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hs_termdoc' is not defined"
     ]
    }
   ],
   "source": [
    "# (3): Import Multinomial Naive Bayes and cross_val_score;\n",
    "# estimate the predictive accuracy of Naive Bayes, using\n",
    "# five-fold crossvalidation.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "ham_spam_scores = cross_val_score(mnb, hs_termdoc.as_matrix(), ham_spam['isspam'], cv=5)\n",
    "print('Mean accuracy: ',sum(ham_spam_scores) / len(ham_spam_scores))\n",
    "ham_spam_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One final question.** By the way, one of the things I did to prepare this data for you was to rebalance it so that there were even numbers of \"spam\" and \"ham\" messages. I did this because we've practiced cross-validation using a simple accuracy measure that doesn't distinguish between \"precision\" and \"recall.\" \n",
    "\n",
    "In the original data set, there were 4,827 ham and 747 spam messages. Why might simple \"accuracy\" be an unreliable way of evaluating a model on an unbalanced dataset like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for part (4): Briefly explain why \"accuracy\" alone isn't an entirely trustworthy measure on unbalanced datasets:\n",
    "\n",
    "Because the more source data the Multinomial NB has, the more accurate it would be. This will disadvantage the spam determination, given that the model is much better at predicting and evaluating the \"ham\" messages. Also, the accuracy on the ham will be much higher due to simple probability, given that, if the algorithm guesses \"ham\" for everything, it will be right vastly more than wrong. So, the prediction could get biased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ham or spam? Part 2.\n",
    "\n",
    "Good old Naive Bayes is strikingly accurate on this task. How is that possible? What words provided the key clues here?\n",
    "\n",
    "There are several ways to answer that question. One way we haven't practiced yet--but that you should know about--is to fit a scikit-learn model, and then use the ```.coef_``` attribute of the model to extract the coefficients that the model itself actually used. The list of numbers will look opaque, but these numbers can be paired with features (the columns in the termdoc matrix you used). For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is purely illustrative; it doesn't need to run in\n",
    "# your notebook (and probably won't).\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(hs_termdoc.as_matrix(), ham_spam['isspam'])\n",
    "# mnb.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we haven't practiced that method, and it's good to have multiple ways of solving a problem. So let's use a method we *have* practiced — Dunning's log likelihood. This is a straightforward way of measuring how much the distribution of a word across two corpora diverges from its EXPECTED frequency -- i.e., the frequency it would have if it were equally distributed over both corpora.\n",
    "\n",
    "Use Dunning's log likelihood to find the ten words most overrepresented in spam, and the ten words most overrepresented in ham.\n",
    "\n",
    "The notebooks for week 4 of the course (\"Representing language geometrically\") could be helpful here, but note that they began by counting words in a different way than we have done above. In week 4, we hadn't introduced the CountVectorizer, so we had to manually create a Counter for each class, holding the number of occurrences for each word.\n",
    "\n",
    "If possible, avoid repeating all the word-counting operations on the ham and spam datasets. Instead, use the term-doc dataframe you already created with the CountVectorizer, and extract the information you need for the signed_dunnings function from that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I have copied a couple of functions you might need from the week 4 notebooks.\n",
    "\n",
    "def signed_dunnings(countsA, totalA, countsB, totalB, word):\n",
    "    ''' This function calculates a signed (+1 / -1)\n",
    "    version of Dunning's log likelihood, for a single word (provided in\n",
    "    the argument \"word\").\n",
    "    \n",
    "    Intuitively, Dunnings log likelihood is a number \n",
    "    that gets larger as the frequencies of the word in our two corpora\n",
    "    diverge from their EXPECTED values -- i.e., the frequencies we would\n",
    "    see if the word were equally distributed. But the Dunnings value also\n",
    "    tends to get larger as the overall frequency of the word increases.\n",
    "    \n",
    "    CountsA and countsB are Counters for the two different corpora, where\n",
    "    keys are words and the values are the # of occurrences of that word\n",
    "    in the corpus.\n",
    "    \n",
    "    This function also requires two additional arguments:\n",
    "    the total number of words in corpus A and corpus B. \n",
    "    \n",
    "    We could calculate those totals inside the function,\n",
    "    but it's faster to calculate them just once, outside the function.\n",
    "    \n",
    "    Also note: the strict definition of Dunnings has no 'sign': it gets bigger\n",
    "    whether a word is overrepresented in A or B. I've edited that so that Dunnings\n",
    "    is positive if overrepresented in A, and negative if overrepresented in B.\n",
    "    '''\n",
    "    \n",
    "    if word not in countsA and word not in countsB:\n",
    "        return 0\n",
    "    \n",
    "    # the raw frequencies of this word in our two corpora\n",
    "    # still doing a little Laplacian smoothing here\n",
    "    a = countsA[word] + 0.1\n",
    "    b = countsB[word] + 0.1\n",
    "    \n",
    "    # now let's calculate the expected number of times this\n",
    "    # word would occur in both if the frequency were constant\n",
    "    # across both\n",
    "    overallfreq = (a + b) / (totalA + totalB)\n",
    "    expectedA = totalA * overallfreq\n",
    "    expectedB = totalB * overallfreq\n",
    "    \n",
    "    # and now the Dunning's formula\n",
    "    dunning = 2 * ((a * math.log(a / expectedA)) + (b * math.log(b / expectedB)))\n",
    "    \n",
    "    if math.isnan(dunning):\n",
    "        print(a, totalA, b, totalB)\n",
    "        user = input('Division by zero error. Are the values above what you expected?')\n",
    "    \n",
    "    if a < expectedA:\n",
    "        return -dunning\n",
    "    else:   \n",
    "        return dunning\n",
    "\n",
    "def headandtail(tuplelist, n):\n",
    "    ''' Returns the top n and bottom n values\n",
    "    in a list of two-tuples, where the first\n",
    "    value of each tuple is numeric.\n",
    "    '''\n",
    "    \n",
    "    tuplelist.sort(reverse = True)\n",
    "    print(\"TOP VALUES:\")\n",
    "    for i in range(n):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "    \n",
    "    print()\n",
    "    print(\"BOTTOM VALUES:\")\n",
    "    lastindex = len(tuplelist) - 1\n",
    "    for i in range(lastindex, lastindex - n, -1):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INCOMPLETE\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Here insert your own code to translate the termdoc data frame\n",
    "# produced by CountVectorizer into the different data structures\n",
    "# expected by our signed_dunnings function.\n",
    "\n",
    "total_spam = Counter()\n",
    "total_ham = Counter()\n",
    "\n",
    "ham_spam_wordlist = []\n",
    "\n",
    "# ham_spam_words = ham_spam.groupby(['text','category'])\n",
    "# ham_spam_isspam = ham_spam_words.aggregate(np.sum)['category']\n",
    "# ham_spam_isspam.head()\n",
    "\n",
    "for word in ham_spam['category']:\n",
    "    if word == 'spam':\n",
    "        ham_spam_wordlist.append(ham_spam['text'])\n",
    "\n",
    "print(len(ham_spam_wordlist))\n",
    "\n",
    "ham_spam_wordlist_joined = \"\".join(ham_spam_wordlist)\n",
    "\n",
    "ham_spam_wordlist_joined\n",
    "\n",
    "# ham_spam_isspam_ = .aggregate(np.sum)['Survived']\n",
    "\n",
    "# for word in ham_spam:\n",
    "#     if word['isspam'] == 1:\n",
    "#         total_spam[word] += 1\n",
    "#     else:\n",
    "#         total_ham[word] += 1\n",
    "\n",
    "\n",
    "# Then loop through all the words in our termdoc data frame,\n",
    "# getting the Dunnings value for each word. Finally, report\n",
    "# the top and bottom 10 words (most common in spam, most\n",
    "# common in ham).\n",
    "\n",
    "# First, a couple of imports you might need:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
