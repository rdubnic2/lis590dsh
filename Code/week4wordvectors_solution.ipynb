{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: The geometry of meaning\n",
    "\n",
    "We're going to explore some basic forms of text analysis, using David Robinson's dataset of tweets made from the account of Donald J. Trump, as well as a dataset of nineteenth-century poetry and fiction, which is divided by date, by genre, and also by reception (whether or not the volume got reviewed in an 'elite' journal).\n",
    "\n",
    "To begin, let's import some modules we're going to need later, and also read in the Trump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tunder/Dropbox/courses/2017datasci/04-Geometry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, csv, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of identifying \"distinctive\" words\n",
    "\n",
    "In this section we'll explore Dunning's log-likelihood, and also think about the strengths and weaknesses of \"distinctive\" words as evidence.\n",
    "\n",
    "First let's glance at the Trump dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>False</td>\n",
       "      <td>9214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 15:20:44</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762669882571980801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>3107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>False</td>\n",
       "      <td>6981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 13:28:20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762641595439190016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>2390</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>False</td>\n",
       "      <td>15724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 00:05:54</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762439658911338496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6691</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>False</td>\n",
       "      <td>19837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 23:09:08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762425371874557952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>False</td>\n",
       "      <td>34051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 21:31:46</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762400869858115588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>11717</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             1   \n",
       "1           1             2   \n",
       "2           2             3   \n",
       "3           3             4   \n",
       "4           4             5   \n",
       "\n",
       "                                                text favorited  favoriteCount  \\\n",
       "0  My economic policy speech will be carried live...     False           9214   \n",
       "1  Join me in Fayetteville, North Carolina tomorr...     False           6981   \n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...     False          15724   \n",
       "3  Michael Morell, the lightweight former Acting ...     False          19837   \n",
       "4  The media is going crazy. They totally distort...     False          34051   \n",
       "\n",
       "  replyToSN              created truncated  replyToSID                  id  \\\n",
       "0       NaN  2016-08-08 15:20:44     False         NaN  762669882571980801   \n",
       "1       NaN  2016-08-08 13:28:20     False         NaN  762641595439190016   \n",
       "2       NaN  2016-08-08 00:05:54     False         NaN  762439658911338496   \n",
       "3       NaN  2016-08-07 23:09:08     False         NaN  762425371874557952   \n",
       "4       NaN  2016-08-07 21:31:46     False         NaN  762400869858115588   \n",
       "\n",
       "   replyToUID                                       statusSource  \\\n",
       "0         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "4         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "        screenName  retweetCount isRetweet retweeted  longitude  latitude  \n",
       "0  realDonaldTrump          3107     False     False        NaN       NaN  \n",
       "1  realDonaldTrump          2390     False     False        NaN       NaN  \n",
       "2  realDonaldTrump          6691     False     False        NaN       NaN  \n",
       "3  realDonaldTrump          6402     False     False        NaN       NaN  \n",
       "4  realDonaldTrump         11717     False     False        NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functions\n",
    "\n",
    "For a lot of the work we do today, we're going to want to construct dictionaries that hold the frequencies of words in different categories: poetry or fiction, Trump-iphone or Trump-android. To do this we'll need to break text into words, count the words in each text, and then add up the counts by category.\n",
    "\n",
    "Let's define some functions that do this. (You can find more polished versions of these functions in the ```nltk``` module.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'and', 'a', 'in', 'is', 'i', 'you', 'of', 'will']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(astring):\n",
    "    ''' Breaks a string into words, and counts them.\n",
    "    Designed so it strips punctuation and lowercases everything,\n",
    "    but doesn't separate hashtags and at-signs.\n",
    "    '''\n",
    "    wordcounts = Counter()\n",
    "    # create a counter to hold the counts\n",
    "    \n",
    "    tokens = astring.split()\n",
    "    for t in tokens:\n",
    "        word = t.strip(',.!?:;-—()<>[]/\"\\'').lower()\n",
    "        wordcounts[word] += 1\n",
    "        \n",
    "    return wordcounts\n",
    "\n",
    "def addcounters(counter2add, countersum):\n",
    "    ''' Adds all the counts in counter2add to countersum.\n",
    "    Because Counters(like dictionaries) are mutable, it\n",
    "    doesn't need to return anything.\n",
    "    '''\n",
    "    \n",
    "    for key, value in counter2add.items():\n",
    "        countersum[key] += value\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for astring in seq_of_strings:\n",
    "        counts = tokenize(astring)\n",
    "        addcounters(counts, vocab)\n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn\n",
    "\n",
    "# Let's test the vocabulary function.\n",
    "vocab = create_vocab(trump['text'], 4000)\n",
    "vocab[0:10]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few more basic functions\n",
    "\n",
    "Once we have a vocabulary, we're going to want to divide our texts into categories, create Counters summing the word frequencies in those categories, and then compare the two Counters to find words that are overrepresented in one category relative to the other.\n",
    "\n",
    "There are several ways we could define \"overrepresented.\" We'll use Robinson's simple log-odds measure, as well as Dunning's log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "two thousand 2000\n",
      "ten 10\n",
      "\n",
      "BOTTOM VALUES:\n",
      "neg one -1\n",
      "zero 0\n"
     ]
    }
   ],
   "source": [
    "def logodds(countsA, countsB, word):\n",
    "    ''' Straightforward.\n",
    "    '''\n",
    "    \n",
    "    odds = (countsA[word] + 1) / (countsB[word] + 1)\n",
    "    \n",
    "    # Why do we add 1 on both sides? Two reasons. The hacky one is \n",
    "    # that otherwise we'll get a division-by-zero error whenever\n",
    "    # word isn't present in countsB. The more principled reason\n",
    "    # is that this technique (called Laplacian smoothing) tends\n",
    "    # to reduce the dramatic disproportion likely to be found in\n",
    "    # very rare words.\n",
    "    \n",
    "    return math.log(odds)\n",
    "\n",
    "def signed_dunnings(countsA, totalA, countsB, totalB, word):\n",
    "    ''' Less straightforward. This function calculates a signed (+1 / -1)\n",
    "    version of Dunning's log likelihood. Intuitively, this is a number \n",
    "    that gets larger as the frequency of the word in our two corpora\n",
    "    diverges from its EXPECTED frequency -- i.e., the frequency it would\n",
    "    have if it were equally distributed over both. But it also tends to get\n",
    "    larger as the raw frequency of the word increases.\n",
    "    \n",
    "    Note that this function requires two additional arguments:\n",
    "    the total number of words in A and B. We could calculate that inside\n",
    "    the function, but it's faster to calculate it just once, outside the function.\n",
    "    \n",
    "    Also note: the strict definition of Dunnings has no 'sign': it gets bigger\n",
    "    whether a word is overrepresented in A or B. I've edited that so that Dunnings\n",
    "    is positive if overrepresented in A, and negative if overrepresented in B.\n",
    "    '''\n",
    "    if word not in countsA and word not in countsB:\n",
    "        return 0\n",
    "    \n",
    "    # the raw frequencies of this word in our two corpora\n",
    "    # still doing a little Laplacian smoothing here\n",
    "    a = countsA[word] + 0.1\n",
    "    b = countsB[word] + 0.1\n",
    "    \n",
    "    # now let's calculate the expected number of times this\n",
    "    # word would occur in both if the frequency were constant\n",
    "    # across both\n",
    "    overallfreq = (a + b) / (totalA + totalB)\n",
    "    expectedA = totalA * overallfreq\n",
    "    expectedB = totalB * overallfreq\n",
    "    \n",
    "    # and now the Dunning's formula\n",
    "    dunning = 2 * ((a * math.log(a / expectedA)) + (b * math.log(b / expectedB)))\n",
    "    \n",
    "    if a < expectedA:\n",
    "        return -dunning\n",
    "    else:   \n",
    "        return dunning\n",
    "\n",
    "# a list of common words is often useful\n",
    "stopwords = {'a', 'an', 'are', 'and', 'but', 'or', 'that', 'this', 'so', \n",
    "             'all', 'at', 'if', 'in', 'i', 'is', 'was', 'by', 'of', 'to', \n",
    "             'the', 'be', 'you', 'were'}\n",
    "\n",
    "# finally, one more function: given a list of tuples like\n",
    "testlist = [(10, 'ten'), (2000, 'two thousand'), (0, 'zero'), (-1, 'neg one'), (8, 'eight')]\n",
    "# we're going to want to sort them and print the top n and bottom n\n",
    "\n",
    "def headandtail(tuplelist, n):\n",
    "    tuplelist.sort(reverse = True)\n",
    "    print(\"TOP VALUES:\")\n",
    "    for i in range(n):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "    \n",
    "    print()\n",
    "    print(\"BOTTOM VALUES:\")\n",
    "    lastindex = len(tuplelist) - 1\n",
    "    for i in range(lastindex, lastindex - n, -1):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "        \n",
    "headandtail(testlist, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Is Dunning's a better measure than logodds for Trump's tweets?\n",
    "\n",
    "Let's put all these functions together to answer that question.\n",
    "\n",
    "I've sketched the outline of a program below in \"pseudocode,\" which\n",
    "describes what needs to be done. Translate that into real Python code, using\n",
    "the functions defined above. First use Robinson's logodds function and try to\n",
    "replicate his results. See what happens if you do (or don't) remove stopwords\n",
    "and tweets that begin with a quote.\n",
    "                                                   \n",
    "Then edit your code to use Dunning's log likelihood. Does that seem to be a better (more revealing) measure of overrepresentation? How would we decide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "#trump2016 273.5841360055747\n",
      "thank 196.81395689400256\n",
      "#makeamericagreatagain 120.28324219640973\n",
      "join 72.69759231558339\n",
      "#americafirst 48.15900365424501\n",
      "#imwithyou 40.72854875396748\n",
      "#votetrump 40.16398066603733\n",
      "tomorrow 33.61963686849273\n",
      "#crookedhillary 28.299637642368594\n",
      "#trumppence16 28.29096593144873\n",
      "soon 27.62623138606114\n",
      "&amp 26.87770273072912\n",
      "tickets 22.455508011339962\n",
      "video 22.08448072761334\n",
      "7pm 22.08448072761334\n",
      "#rncincle 20.018560530293765\n",
      "#inprimary 20.018560530293765\n",
      "#maga 19.283927014638802\n",
      "new 17.79952270616385\n",
      "indiana 17.730138764444213\n",
      "pennsylvania 16.154464660809687\n",
      "#wiprimary 15.893025142878843\n",
      "#fitn 15.893025142878843\n",
      "safe 15.150939231055148\n",
      "officers 13.83449313808438\n",
      "\n",
      "BOTTOM VALUES:\n",
      "@realdonaldtrump -96.7586196380446\n",
      "@megynkelly -16.56740161733122\n",
      "more -15.782782083381733\n",
      "because -13.567850903933373\n",
      "job -13.396043113762923\n",
      "win -13.31962104674283\n",
      "trump -12.813499152342672\n",
      "big -11.93248427605737\n",
      "a.m -11.28010329618746\n",
      "very -11.060232026337701\n",
      "badly -10.425110057403096\n",
      "two -10.425110057403096\n",
      "@cnn -10.260274042336071\n",
      "against -9.642121312108062\n",
      "crazy -9.571276780176985\n",
      "p.m -9.571276780176985\n",
      "u -9.571276780176985\n",
      "weak -9.571276780176985\n",
      "had -9.392239375687556\n",
      "republican -9.392239375687556\n",
      "than -9.392239375687556\n",
      "media -9.16164373266507\n",
      "spent -8.718802942765198\n",
      "that's -8.718802942765198\n",
      "beat -8.634346381091559\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(trump['text'], 5000)\n",
    "vocab = list(set(vocab) - stopwords)\n",
    "\n",
    "numrows = trump.shape[0]\n",
    "android = Counter()\n",
    "iphone = Counter()\n",
    "\n",
    "#skipped = 0\n",
    "for i in range(numrows):\n",
    "    # if trump['text'][i].startswith('\"'):\n",
    "        #skipped += 1\n",
    "        # continue\n",
    "        \n",
    "    counts = tokenize(trump['text'][i])\n",
    "    if 'iphone' in trump['statusSource'][i]:\n",
    "        addcounters(counts, iphone)\n",
    "    elif 'android' in trump['statusSource'][i]:\n",
    "        addcounters(counts, android)\n",
    "#print(skipped)\n",
    "iphonesum = sum(iphone.values())\n",
    "androidsum = sum(android.values())\n",
    "\n",
    "tuplelist = []\n",
    "\n",
    "for word in vocab:\n",
    "    g = signed_dunnings(iphone, iphonesum, android, androidsum, word)\n",
    "    # g = logodds(iphone, android, word)\n",
    "    tuplelist.append((g, word))\n",
    "\n",
    "headandtail(tuplelist, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Apply the same methods to a more literary dataset.\n",
    "\n",
    "I've also provided a dataset of roughly 1026 snippets from nineteenth-century poetry and fiction. The code below should read it in. Run that, then copy and paste the code you worked up for Trump, and edit it so it provides the most distinctive words for poetry (versus fiction).\n",
    "\n",
    "If we have time, it may also be worth distinguishing poetry reviewed in elite journals from poetry that wasn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>reception</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908</td>\n",
       "      <td>Robins, Elizabeth,</td>\n",
       "      <td>The convert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>looked like decent artisans, but more who bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton,</td>\n",
       "      <td>The coming race</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>called the \" Easy Time \" (with which what I ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1872</td>\n",
       "      <td>Butler, Samuel,</td>\n",
       "      <td>Erewhon, or, Over the range</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>the curtain ; on this I let it drop and retrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1900</td>\n",
       "      <td>Barrie, J. M.</td>\n",
       "      <td>Tommy and Grizel</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>at you !\" he said. \"Dear eyes, \" said she. \"Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873</td>\n",
       "      <td>Ritchie, Anne Thackeray,</td>\n",
       "      <td>Old Kensington</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>furious; I have not dared tell her, poor creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                         author                        title    genre  \\\n",
       "0  1908             Robins, Elizabeth,                  The convert  fiction   \n",
       "1  1871  Lytton, Edward Bulwer Lytton,              The coming race  fiction   \n",
       "2  1872                Butler, Samuel,  Erewhon, or, Over the range  fiction   \n",
       "3  1900                  Barrie, J. M.             Tommy and Grizel  fiction   \n",
       "4  1873       Ritchie, Anne Thackeray,               Old Kensington  fiction   \n",
       "\n",
       "  reception                                               text  \n",
       "0     elite  looked like decent artisans, but more who bore...  \n",
       "1     elite  called the \" Easy Time \" (with which what I ma...  \n",
       "2     elite  the curtain ; on this I let it drop and retrea...  \n",
       "3     elite  at you !\" he said. \"Dear eyes, \" said she. \"Th...  \n",
       "4     elite  furious; I have not dared tell her, poor creat...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "poefic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code deleted, because it's now a homework problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using corpora to create a \"meaning space.\"\n",
    "\n",
    "Contrasting two corpora can be revealing, but sometimes we want to think about the relations between individual texts or words. To do that, we often represent them as vectors in a multi-dimensional space.\n",
    "\n",
    "The simplest way to do this is to create a DataFrame where rows are documents and columns are word — a document-term matrix. Here's a function that does that. It requires a pre-defined vocabulary (list of words) as well as a list (or numpy vector) of texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_term_matrix(vocab, textvector):\n",
    "    ''' Transform the textvector into a document-term matrix\n",
    "    with one column for each word in vocab.\n",
    "    '''\n",
    "    \n",
    "    n = len(textvector)\n",
    "    vocabset = set(vocab)\n",
    "    # making a set so we can check membership quickly;\n",
    "    # it's much faster in a set than in a list\n",
    "    \n",
    "    termdictionary = dict()\n",
    "    for word in vocab:\n",
    "        termdictionary[word] = np.zeros(n)\n",
    "    for i, text in enumerate(textvector):\n",
    "        counts = tokenize(text)\n",
    "        for word, count in counts.items():\n",
    "            if word in vocabset:\n",
    "                termdictionary[word][i] += count\n",
    "    \n",
    "    dtmatrix = pd.DataFrame(termdictionary, columns = vocab)\n",
    "    return dtmatrix\n",
    "\n",
    "# A nice arcane trick to perform on a document-term matrix\n",
    "# is to squash it into a smaller number of dimensions. This\n",
    "# often reveals relationships between words that don't\n",
    "# necessarily, literally occur together. The technique is called\n",
    "# Latent Semantic Analysis.\n",
    "\n",
    "def lsa_matrix(dtmatrix, vocab, number_of_dimensions):\n",
    "    lsa = TruncatedSVD(number_of_dimensions, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(dtmatrix)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    lsamatrix = pd.DataFrame(lsa.components_, columns = vocab)\n",
    "    \n",
    "    return lsamatrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.sum(vector1 * vector2)\n",
    "    # we assume these are numpy vectors and can be\n",
    "    # multiplied elementwise\n",
    "    \n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8531 15662\n"
     ]
    }
   ],
   "source": [
    "print(iphonesum, androidsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = create_vocab(poefic['text'], 2500)\n",
    "dtm = doctermmatrix(vocab, poefic['text'])\n",
    "lsa = make_lsa_matrix(dtm, vocab, 25)\n",
    "\n",
    "def find_match(amatrix, vocab, user_word):\n",
    "    vocabset = set(vocab)\n",
    "    numcol = amatrix.shape[1]\n",
    "    user_vector = amatrix[user_word]\n",
    "    tuplelist = []\n",
    "    for i in range(numcol):\n",
    "        cossim = cosine_similarity(amatrix.iloc[ : , i], user_vector)\n",
    "        tuplelist.append((cossim, vocab[i]))\n",
    "    headandtail(tuplelist, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word? herself\n",
      "TOP VALUES:\n",
      "herself 1.0000000000000002\n",
      "she 0.7716452092647031\n",
      "daughters 0.5875505691208556\n",
      "impossible 0.5768217529798366\n",
      "possible 0.5702868892412817\n",
      "slight 0.5455494133385587\n",
      "drawing 0.5351924949952669\n",
      "daughter 0.5234257917168922\n",
      "husband 0.5221905605041337\n",
      "rooms 0.521308013989169\n",
      "\n",
      "BOTTOM VALUES:\n",
      "tower -0.5465893529018647\n",
      "lights -0.5118834629986829\n",
      "sounds -0.44183523251151824\n",
      "forest -0.41912245104573653\n",
      "hills -0.41777580862271546\n",
      "hill -0.4089810930096797\n",
      "echoes -0.4054595882372723\n",
      "swift -0.4050970804953546\n",
      "bark -0.3980934359379713\n",
      "streams -0.39759991597819794\n"
     ]
    }
   ],
   "source": [
    "user_word = input('word? ')\n",
    "find_match(lsa, vocab, user_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
