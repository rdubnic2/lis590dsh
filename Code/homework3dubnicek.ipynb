{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rdubnic2/Documents/lis590dsh/Code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'fivethirtyeight', 'bechdel.csv')\n",
    "bechdel = pd.read_csv(relativepath)\n",
    "\n",
    "# bringing functions over from in-class exercises\n",
    "def dot(vectorA, vectorB):\n",
    "    ''' Calculates the dot product of two vectors; in other words, the\n",
    "    sum of the pairwise products of A and B.\n",
    "    '''\n",
    "    assert len(vectorA) == len(vectorB)\n",
    "    # If that's not true, this function should just fail.\n",
    "    n = len(vectorA)\n",
    "    dotproduct = 0    \n",
    "    for i in range(n):\n",
    "        dotproduct += vectorA[i] * vectorB[i]\n",
    "        \n",
    "    return dotproduct\n",
    "    # this is the value the function will return\n",
    "\n",
    "def covar(vecA, vecB):\n",
    "    assert len(vecA) == len(vecB)\n",
    "    n = len(vecA)\n",
    "    diffA = vecA - np.mean(vecA)\n",
    "    diffB = vecB - np.mean(vecB)\n",
    "    cov = dot(diffA, diffB) / (n-1)\n",
    "    \n",
    "    return cov\n",
    "        \n",
    "def pearson(vec1, vec2):\n",
    "    x = covar(vec1, vec2)\n",
    "    stdev = x / (np.std(vec1) * np.std(vec2))\n",
    "    return stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining In-Class Exercises\n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "Write Python code to test the significance of a correlation between two arbitrary vectors. It should return r, the correlation coefficient, as well as p, the fraction of times we get a larger correlation than r out of (say) a hundred tries with a randomly-permuted relationship between the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r =  -1.0857779238\n",
      "p =  0\n"
     ]
    }
   ],
   "source": [
    "# Code for Exercise 6\n",
    "\n",
    "# I'm not entirely sure what is being asked here, as it's hard to tell from the wording of the question,\n",
    "#  especially given my near complete lack of mathematical knowledge. But this is my shot:\n",
    "\n",
    "def randcorr(vecX, vecY):\n",
    "    import random\n",
    "    x = pearson(vecX, vecY)\n",
    "    print('r = ', + x)\n",
    "    count = 0\n",
    "    for x in range(50):\n",
    "        random.shuffle(vecX)\n",
    "        # print(vecX)\n",
    "        if pearson(vecX, vecY) > x:\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 0\n",
    "        # print(count)\n",
    "    for y in range(50):\n",
    "        random.shuffle(vecY)\n",
    "        # print(vecY)\n",
    "        if pearson(vecX, vecY) > x:\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 0\n",
    "        # print(count)\n",
    "    print('p = ', + count)\n",
    "\n",
    "vecC = [43,1.6,71,12.3,5.7]\n",
    "vecD = [0.875,7.1,.888,9.987,11]\n",
    "\n",
    "randcorr(vecC, vecD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Use the function you've defined to test the significance of correlations between budget and international gross returns for our movies, and then also between budget and ROI.\n",
    "\n",
    "You may need to remove rows in the dataframe where gross returns are missing. The ```isnull()``` function will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>imdb</th>\n",
       "      <th>title</th>\n",
       "      <th>test</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>binary</th>\n",
       "      <th>budget</th>\n",
       "      <th>domgross</th>\n",
       "      <th>intgross</th>\n",
       "      <th>code</th>\n",
       "      <th>budget_2013</th>\n",
       "      <th>domgross_2013</th>\n",
       "      <th>intgross_2013</th>\n",
       "      <th>period_code</th>\n",
       "      <th>decade_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>tt1711425</td>\n",
       "      <td>21 &amp; Over</td>\n",
       "      <td>notalk</td>\n",
       "      <td>notalk</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380.0</td>\n",
       "      <td>42195766.0</td>\n",
       "      <td>2013FAIL</td>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380.0</td>\n",
       "      <td>42195766.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>tt1343727</td>\n",
       "      <td>Dredd 3D</td>\n",
       "      <td>ok-disagree</td>\n",
       "      <td>ok</td>\n",
       "      <td>PASS</td>\n",
       "      <td>45000000</td>\n",
       "      <td>13414714.0</td>\n",
       "      <td>40868994.0</td>\n",
       "      <td>2012PASS</td>\n",
       "      <td>45658735</td>\n",
       "      <td>13611086.0</td>\n",
       "      <td>41467257.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>tt2024544</td>\n",
       "      <td>12 Years a Slave</td>\n",
       "      <td>notalk-disagree</td>\n",
       "      <td>notalk</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>20000000</td>\n",
       "      <td>53107035.0</td>\n",
       "      <td>158607035.0</td>\n",
       "      <td>2013FAIL</td>\n",
       "      <td>20000000</td>\n",
       "      <td>53107035.0</td>\n",
       "      <td>158607035.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>tt1272878</td>\n",
       "      <td>2 Guns</td>\n",
       "      <td>notalk</td>\n",
       "      <td>notalk</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>61000000</td>\n",
       "      <td>75612460.0</td>\n",
       "      <td>132493015.0</td>\n",
       "      <td>2013FAIL</td>\n",
       "      <td>61000000</td>\n",
       "      <td>75612460.0</td>\n",
       "      <td>132493015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>tt0453562</td>\n",
       "      <td>42</td>\n",
       "      <td>men</td>\n",
       "      <td>men</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>40000000</td>\n",
       "      <td>95020213.0</td>\n",
       "      <td>95020213.0</td>\n",
       "      <td>2013FAIL</td>\n",
       "      <td>40000000</td>\n",
       "      <td>95020213.0</td>\n",
       "      <td>95020213.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       imdb             title             test clean_test binary  \\\n",
       "0  2013  tt1711425         21 & Over           notalk     notalk   FAIL   \n",
       "1  2012  tt1343727          Dredd 3D      ok-disagree         ok   PASS   \n",
       "2  2013  tt2024544  12 Years a Slave  notalk-disagree     notalk   FAIL   \n",
       "3  2013  tt1272878            2 Guns           notalk     notalk   FAIL   \n",
       "4  2013  tt0453562                42              men        men   FAIL   \n",
       "\n",
       "     budget    domgross     intgross      code  budget_2013  domgross_2013  \\\n",
       "0  13000000  25682380.0   42195766.0  2013FAIL     13000000     25682380.0   \n",
       "1  45000000  13414714.0   40868994.0  2012PASS     45658735     13611086.0   \n",
       "2  20000000  53107035.0  158607035.0  2013FAIL     20000000     53107035.0   \n",
       "3  61000000  75612460.0  132493015.0  2013FAIL     61000000     75612460.0   \n",
       "4  40000000  95020213.0   95020213.0  2013FAIL     40000000     95020213.0   \n",
       "\n",
       "   intgross_2013  period_code  decade_code  \n",
       "0     42195766.0          1.0          1.0  \n",
       "1     41467257.0          1.0          1.0  \n",
       "2    158607035.0          1.0          1.0  \n",
       "3    132493015.0          1.0          1.0  \n",
       "4     95020213.0          1.0          1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for Exercise 7\n",
    "\n",
    "# print(bechdel['intgross_2013'].isnull())\n",
    "adjbech = bechdel.fillna(0)\n",
    "adjbech.head()\n",
    "\n",
    "# randcorr(adjbech['budget_2013'], adjbech['intgross_2013'])\n",
    "# randcorr(adjbech['budget_2013'], adjbech['roi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Functions and Hypothesis-Testing\n",
    "\n",
    "This homework begins by reviewing some things we accomplished in class, and defining some functions. Then it asks you to apply those functions to the Bechdel dataset. We won't fully replicate the data analysis performed in [the Bechdel article,](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/) and spelled out (using R) in [this online analysis.](https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html) But we're going to build some elements of the process from the ground up, so that we genuinely understand what's going on â€” especially where hypothesis testing is involved.\n",
    "\n",
    "I should say up front that the approach we're taking to hypothesis-testing here is only one of several possible approaches; it's based on what statisticians call a *frequentist* theory of statistics. *Bayesian* statistics provide an alternative way of reasoning about these questions; we'll glance at that in a couple of weeks.\n",
    "\n",
    "To start with, let's test the significance of a correlation. We saw in class that movies that fail to pass the Bechdel test have lower median return-on-investment than those that do pass. This hints that movie studios may be controlled by a kind of sexism that is not only unjust, and not only not profitable, but actually self-destructive and money-*losing.* \n",
    "\n",
    "On the other hand, I mentioned in class that there's a potential *confounding variable* in this argument. Movies with lower budgets generally have higher ROI. So perhaps movies that pass the Bechdel test have higher ROI simply *because* they have lower budgets? We won't completely resolve that question below, but we'll build up to it. To begin with, is the relationship between budget and return-on-investment actually meaningful, or would we be likely to see relations of that kind emerge by chance?\n",
    "\n",
    "To test that, we'll start by reconstructing the functions we need for a correlation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our version:  0.766723820184\n",
      "Official version:  (0.76672382018354224, 0.015931317092922143)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We start (above) by importing the usual suspects.\n",
    "\n",
    "# Read the functions below to get a sense of how you can build up a complex\n",
    "# function by breaking it into elements.\n",
    "\n",
    "def dot(vectorA, vectorB):\n",
    "    ''' Calculates the dot product of two vectors; in other words, the\n",
    "    sum of the pairwise products of A and B.\n",
    "    '''\n",
    "    assert len(vectorA) == len(vectorB)\n",
    "    dotproduct = np.sum(vectorA * vectorB)\n",
    "    # that's the simpler version of a dot product, using\n",
    "    # numpy's automatic elementwise multiplication of vectors\n",
    "    \n",
    "    return dotproduct\n",
    "\n",
    "def covariance(vecA, vecB):\n",
    "    ''' Calculates the covariance of two vectors; in other words, the\n",
    "    tendency for A to be above its average when B is also above average,\n",
    "    and below when B is also below.\n",
    "    '''\n",
    "    \n",
    "    n = len(vecA)\n",
    "    if n > 1:\n",
    "        diffA = vecA - np.mean(vecA)\n",
    "        diffB = vecB - np.mean(vecB)\n",
    "        cov = dot(diffA, diffB) / n\n",
    "        \n",
    "        # In class we were using n-1 here, which gives sample covariance.\n",
    "        # The official function uses n (population covariance), so that's\n",
    "        # what I've substituted. It won't make much difference as we scale up\n",
    "        # to large n.\n",
    "        \n",
    "        return cov\n",
    "    else:\n",
    "        print('Error! covariance has no meaning if vectors have less than two elements.')\n",
    "        return float('NaN')\n",
    "\n",
    "def pearson_correlation(vecA, vecB):\n",
    "    ''' Calculates the Pearson correlation coefficient, r, for two\n",
    "    vectors.\n",
    "    '''\n",
    "    \n",
    "    cov = covariance(vecA, vecB)\n",
    "    \n",
    "    if np.std(vecA) == 0 or np.std(vecB) == 0:\n",
    "        print('Error! statistic undefined if standard deviations are zero.')\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        pearson = cov / (np.std(vecA) * np.std(vecB))\n",
    "        return pearson\n",
    "\n",
    "    \n",
    "vectorA = np.array([1, 1, 3, 4, 5, 2, 7, 8, 7])\n",
    "vectorB = np.array([0, 2, 4, 5, 2, 6, 8, 9, 6])\n",
    "\n",
    "print('Our version: ', pearson_correlation(vectorA, vectorB))\n",
    "\n",
    "# Let's test that we're getting this roughly right\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print('Official version: ', pearsonr(vectorA, vectorB))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of what it means to calculate a p-value\n",
    "\n",
    "It appears that we're calculating Pearson's r the same way the scipy module does it. But they provide an additional piece of information: the p-value, or the number of times you would get an equally large (or small!) r value if the data was random, with no relation between the vectors. Let's test that in an intuitive way.\n",
    "\n",
    "We'll randomize the data and see how often we do get an equally large (or small) r value. Here's how you can randomize a list (or a numpy vector / pandas Series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "vectorC = [1, 2, 3, 4, 5]\n",
    "random.shuffle(vectorC)\n",
    "print(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to repeatedly test correlation between our two vectors while randomizing one of them. The number of random correlations that are equal to or greater than our r value will tell us how often this could happen by chance.\n",
    "\n",
    "A Pearson's correlation coefficient can range from -1 (perfect inverse correlation) to 1 (perfect positive correlation). We'll use the *absolute value* of all the r values (i.e, forcing them all to be positive), because we didn't actually start with a hypothesis about whether this correlation was negative or positive. So *any* strong correlation (say an inverse correlation of -0.77) should count as evidence that \"a statistic this extreme could have occurred by chance.\" To test absolute magnitude rather than sign, we'll transform everything using the ```abs()``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76672382018354246, 0.015)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def permutation_test_of_pearson_2tailed(vectorA, vectorB):\n",
    "    r = pearson_correlation(vectorA, vectorB)\n",
    "    absolute_r = abs(r)\n",
    "    \n",
    "    copiedA = np.array(vectorA)\n",
    "    copiedB = np.array(vectorB)\n",
    "    # we do that to avoid permanently shuffling vectorA\n",
    "    \n",
    "    random_r_values = list()\n",
    "    \n",
    "    number_of_tests = 1000\n",
    "    \n",
    "    for i in range(number_of_tests):\n",
    "        random.shuffle(copiedA)\n",
    "        random.shuffle(copiedB)\n",
    "        random_r = pearson_correlation(copiedA, copiedB)\n",
    "        random_r_values.append(abs(random_r))\n",
    " \n",
    "    # how many random r values are greater than or equal to the\n",
    "    # one we found in the actual vectors?\n",
    "    # to find out, we'll use the enumerate function,\n",
    "    # which returns elements of a list paired with\n",
    "    # their numeric index in the list\n",
    "    \n",
    "    random_r_values.sort(reverse = True)\n",
    "    for index, random_val in enumerate(random_r_values):\n",
    "        if random_val < absolute_r:\n",
    "            break\n",
    "    p = index / number_of_tests\n",
    "    \n",
    "    return r, p\n",
    "              \n",
    "permutation_test_of_pearson_2tailed(vectorA, vectorB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p value calulated by our function won't be precisely equal to the value calculated by pearsonr, because there's an element of randomness here. But they will usually be in the same ballpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Editing the permutation test.\n",
    "\n",
    "Suppose we had started by predicting that the correlation between these two vectors would be *positive.* Then, in principle, we would only want to know how often a raw correlation coefficient greater than the observed value could occur by chance. Random negative correlations would not do anything to weaken our confidence.\n",
    "\n",
    "Copy and paste the code of the 2-tailed permutation test, and then edit it to make it 1-tailed. In other words, now you only want to ask how often the random Pearson's correlation is actually *larger* than the observed value, not how often it's *more extreme* than the observed value.\n",
    "\n",
    "If this is opaque, you might want to read [this account of the difference between one- and two-tailed tests.](https://en.m.wikipedia.org/wiki/One-_and_two-tailed_tests)\n",
    "\n",
    "The edit required is actually very simple; I just want to give you an occasion to read through the function and understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76672382018354246, 0.012)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for problem 1.\n",
    "\n",
    "def permutation_test_of_pearson_1tailed(vectorA, vectorB):\n",
    "    r = pearson_correlation(vectorA, vectorB)\n",
    "    # removed use of abs() to force r positive\n",
    "    copiedA = np.array(vectorA)\n",
    "    copiedB = np.array(vectorB)\n",
    "    # we do that to avoid permanently shuffling vectorA\n",
    "    \n",
    "    random_r_values = list()\n",
    "    \n",
    "    number_of_tests = 1000\n",
    "    \n",
    "    for i in range(number_of_tests):\n",
    "        random.shuffle(copiedA)\n",
    "        random.shuffle(copiedB)\n",
    "        random_r = pearson_correlation(copiedA, copiedB)\n",
    "        random_r_values.append(random_r) # removed abs() to force appended r values to be positive\n",
    " \n",
    "    # how many random r values are greater than or equal to the\n",
    "    # one we found in the actual vectors?\n",
    "    # to find out, we'll use the enumerate function,\n",
    "    # which returns elements of a list paired with\n",
    "    # their numeric index in the list\n",
    "    \n",
    "    random_r_values.sort(reverse = True)\n",
    "    for index, random_val in enumerate(random_r_values):\n",
    "        if random_val < r: # replaced call to abs(r) with just r, to reflect potential negatives\n",
    "            break\n",
    "    p = index / number_of_tests\n",
    "    \n",
    "    return r, p\n",
    "              \n",
    "permutation_test_of_pearson_1tailed(vectorA, vectorB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Significance of correlation between budget and ROI\n",
    "\n",
    "Read in the Bechdel dataset (using the code below). Note that this code filters the dataset in several ways: for instance, it drops all rows where NaNs occur. I've done this for you using the .dropna() method. In this case, we've only dropped rows where NaNs occur in columns we're going to use (that's the significance of the \"subset\" argument).\n",
    "\n",
    "I also drop all films before 1990, in order to replicate the analysis at https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rdubnic2/Documents/lis590dsh/Code\n",
      "\n",
      "Original shape:  (1794, 15)\n",
      "After dropping rows with NaN:  (1776, 15)\n",
      "After dropping rows before 1990:  (1600, 15)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'fivethirtyeight', 'bechdel.csv')\n",
    "bechdel = pd.read_csv(relativepath)\n",
    "print(\"Original shape: \", bechdel.shape)\n",
    "bechdel = bechdel.dropna(subset = ['domgross_2013', 'budget_2013', 'intgross_2013'])\n",
    "print(\"After dropping rows with NaN: \", bechdel.shape)\n",
    "bechdel = bechdel[bechdel['year'] > 1989]\n",
    "print(\"After dropping rows before 1990: \", bechdel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out whether it's really meaningfully true that films with bigger budgets have lower return-on-investment. \n",
    "\n",
    "To figure this out, you'll need to start by creating a return-on-investment column. For our purposes let's focus on *domestic* ROI, which we'll calculate as the domestic gross receipts (inflation-corrected to 2013 dollars) divided by the film's budget (also inflation-corrected to 2013 dollars).\n",
    "\n",
    "Let's also adopt the conventional definition of statistical significance as p < 0.05, or roughly \"if the null hypothesis were true, a test statistic equally extreme could have occurred by chance more than 5% of the time.\"\n",
    "\n",
    "Given that definition, find out whether domestic return on investment has a significant (positive or negative) correlation with budget. Use 2013 dollars in both cases. In principle, you could test significance of the correlation using *either* the official ```pearsonr``` function or our ```permutation_test_of_pearson_2tailed``` function. But use both, to see how well they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our version:  (-0.11954316394556917, 0.0)\n",
      "Official version:  (-0.1195431639455691, 1.6251759110499532e-06)\n"
     ]
    }
   ],
   "source": [
    "# Code for problem 2\n",
    "\n",
    "bechdel['domroi_2013'] = (bechdel['domgross_2013'] / bechdel['budget_2013'])\n",
    "bechdel.head()\n",
    "\n",
    "print('Our version: ', permutation_test_of_pearson_2tailed(bechdel['domroi_2013'], bechdel['budget_2013']))\n",
    "\n",
    "print('Official version: ', pearsonr(bechdel['domroi_2013'], bechdel['budget_2013']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be a correlation between domestic ROI and budget size for films post-1990. It appears to be a slight negative correlation, giving credence to the idea that films with larger budgets have a slight tendency to have smaller ROIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Difference between medians\n",
    "\n",
    "Let's also test the significance of the difference we observed between median ROI for films that do or don't pass the Bechdel test. \n",
    "\n",
    "First, calculate the median domestic return on investment for films that do, or don't, pass the Bechdel test (using the ```binary``` column to divide them).\n",
    "\n",
    "Then import the function ```median_test``` from ```scipy.stats``` (the same place we got pearsonr), and use it to assess the significance of the difference between medians. \n",
    "\n",
    "For a [full explanation of that function,](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.median_test.html) see the scipy documentation. You'll need to read this in order to figure out, for instance, how many arguments to send to the function.\n",
    "\n",
    "Basically, this function is doing something analogous to our permutation-test for significance of correlations. It's pooling all the data (both vectors of ROI figures, for films that pass the Bechdel test and films that fail) and calculating a \"grand median\" for the whole group. Then it asks how often we would randomly draw two groups of films from that master set that are as differently-distributed relative to the \"grand median\" as the two groups we actually passed in as arguments.\n",
    "\n",
    "It may be a useful reminder that when you index a DataFrame with single label like ```bechdel['budget_2013']```, you're selecting a column. But when you index using a list or vector of Boolean true/false values ```bechdel[bechdel['binary'] == 'PASS']```, you're selecting rows where that condition is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.3483067028439351,\n",
       " 0.0022319158626105144,\n",
       " 1.2869639280222738,\n",
       " array([[403, 397],\n",
       "        [341, 459]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for problem 3\n",
    "\n",
    "grouped_bechdel = bechdel.groupby('binary')\n",
    "bmean = grouped_bechdel.aggregate(np.mean)['domroi_2013']\n",
    "bmedian = grouped_bechdel.aggregate(np.median)['domroi_2013']\n",
    "bcount = grouped_bechdel.count()['domroi_2013']\n",
    "\n",
    "from scipy.stats import median_test\n",
    "bechpass = bechdel[bechdel['binary'] == 'PASS']\n",
    "bechfail = bechdel[bechdel['binary'] == 'FAIL']\n",
    "\n",
    "median_test(bechpass['domroi_2013'], bechfail['domroi_2013'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Discussion and conclusions.\n",
    "\n",
    "We haven't yet answered the question we started out with. To work it out completely in this assignment would require a few tricks I haven't yet taught you using Pandas, but let's leap forward a bit by relying on [the published analysis of this data I mentioned before.](https://mran.microsoft.com/web/packages/fivethirtyeight/vignettes/bechdel.html)\n",
    "\n",
    "This will be a little tricky to read, because they use R rather than Python/Pandas. But the principles are the same, and the significance of p-values, for instance, should now be legible to you, even if the format is a little different.\n",
    "\n",
    "How much can we infer from this evidence about sexism in the movie industry?\n",
    "Do you notice any important data analysis tricks used in the published analysis that we haven't covered yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Write your conclusions in this Markdown cell. A paragraph of 5-8 sentences should be fine.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## _Conclusions_\n",
    "\n",
    "I'm not entirely sure we can infer a huge amount about sexism in the movie industry from this analysis, to be honest, but there are some very general findings that are concluded. It looks like movies with larger budgets do indeed tend to have a smaller ROI, and films that pass the Bechdel test do tend to have smller budgets. This could possibly mean either that studios/financiers are less likely to finance films that pass the Bechdel test, that films that pass the test are less likely to opt for studio financing, or even that they tend to be in genres that are less budget-intensive (i.e. drama, comedy, romance, etc.). Because of this, it's still unclear whether films with larger budgets ensured they passed the test would see larger ROIs. However, we are seeing that this effect only applies to U.S. and Canada ROIs, with international ROI evening out over the long-term.\n",
    "\n",
    "All-in-all, it's likely fair to say that films that pass the Bechdel test do not have the level of funding available to them as those that do not (for whatever reason), and still tend to do very well in ROI. Hollywood is clearly _very_ sexist, in my opinion, but I'm not sure a lot of specifics can be pulled from this beyond these two conclusions.\n",
    "\n",
    "The most interesting trick in the linked analysis is breaking out a return on investment per dollar spent figure, which really clarifies this analysis, for me. Rather than dealing with means and mediand directly, then comparing where the films fall, it's a much more elegant and simple comparison to show one numerical figure to illustrate all of the arguments. Using their plot style/mechanism is also helpful for identifying the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
