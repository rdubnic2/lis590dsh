{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Having written [a lot of prose to explain topic modeling](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) elsewhere, I won't repeat myself at length here.\n",
    "\n",
    "Suffice it to say that this notebook demonstrates an implementation of LDA in python, using the ```gensim``` module.\n",
    "\n",
    "Topic modeling is an area where sheer compute power starts to matter more than it has in most of our other work, and I don't think ```gensim``` is necessarily the fastest implementation. If you wanted to apply topic modeling to a large corpus, it might be worthwhile figuring out how to use gensim in a \"distributed\" way, or exploring another implementation, such as [```MALLET.```](http://mallet.cs.umass.edu) MALLET is the most commonly-used implementation in digital humanities, and there's [a good Programming Historian tutorial.](http://programminghistorian.org/lessons/topic-modeling-and-mallet) However, MALLET requires Java, and I wanted to limit the number of installation problems we confront.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# You may not have the stopwords downloaded yet.\n",
    "# You can comment this out after it runs once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a corpus\n",
    "\n",
    "I've provided three corpora: ```tinywikicorpus.csv```, ```smallwikicorpus.csv```, and ```mediumwikicorpus.csv.```\n",
    "\n",
    "This stuff gets compute-intensive pretty fast, so let's start with the small one. This has 250 Wikipedia pages, each on a separate line of the file -- and only the first 250 words of each page. The tiny corpus has 160 words of 160 pages; the medium corpus has 400 words from 400 pages.\n",
    "\n",
    "Obviously, this is not a huge corpus! But in real-life applications, you have to distribute topic modeling over multiple cores, and even then it's common to wait several hours for a result. That doesn't adapt very well to a classroom experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n"
     ]
    }
   ],
   "source": [
    "# Very simply, reading the corpus from a text file.\n",
    "# Each page is on a separate line.\n",
    "\n",
    "# relativepath = os.path.join('..', 'data', 'smallwikicorpus.txt')\n",
    "# relativepath = os.path.join('..', 'data', 'mediumwikicorpus.txt')\n",
    "relativepath = os.path.join('..', 'data', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "fictioncorpus = [' '.join(x.split()) for x in poefic.text] # or slice it more if you want, or don't\n",
    "\n",
    "# print(fictioncorpus[0])\n",
    "print(len(fictioncorpus))\n",
    "\n",
    "# wikicorpus = []\n",
    "# with open(relativepath, encoding = 'utf-8') as f:\n",
    "#     for line in f:\n",
    "#         wikicorpus.append(line.strip())\n",
    "        \n",
    "# print(wikicorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the corpus for topic modeling\n",
    "\n",
    "In part this is a simple tokenizing job. We have represented Wikipedia pages as single strings; gensim is going to expect each document to be a *list* of words. So we need to split the document into words.\n",
    "\n",
    "But in the process of doing that, we also want to get rid of extremely common words, which make a topic model difficult to read and interpret.\n",
    "\n",
    "To do this, we create a list of \"stopwords.\" We also remove punctuation, and lowercase everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clean_corpus contains 1027 texts.\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# We're going to borrow a list of stopwords from nltk.\n",
    "\n",
    "# This list of \"stopwords\" removed from the corpus is not\n",
    "# a trivial, generic decision; your choice of stopwords can\n",
    "# in practice significantly affect the result. Here's a place where\n",
    "# the open-ended character of an unsupervised learning algorithm\n",
    "# becomes tricky.\n",
    "\n",
    "# stopwords = {'a', 'an', 'the', 'of', 'and', 'in', 'to', 'by', 'on', 'for', 'it', 'at', 'me', 'from', 'with', '.', ','}\n",
    "# in case you can't access nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stopped = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "stopped = stopped.union(punctuation)\n",
    "\n",
    "more_stops = {\"paul\", \"john\", \"jack\", \"\\'s\", \"nt\",\n",
    "              \"``\", \"\\'the\", \";\", '“', 'pb', \"mary\", \n",
    "              \"henry\", \"arthur\", \"polly\", \"alice\", \n",
    "              \"jane\", \"jean\", \"michael\", \"harold\",\n",
    "             \"tom\", \"richard\"}\n",
    "# When you're topic-modeling fiction, personal names\n",
    "# present a special problem.\n",
    "\n",
    "stopped = stopped.union(more_stops)\n",
    "punctuation.add('“')\n",
    "punctuation.add('”')\n",
    "punctuation.add('—')\n",
    "\n",
    "def strip_punctuation(atoken):\n",
    "    global punctuation\n",
    "    punct_stripped = ''.join([ch for ch in atoken if ch not in punctuation])\n",
    "    return punct_stripped\n",
    "\n",
    "def clean_text(atext):\n",
    "    global stopped\n",
    "    clean_version = [strip_punctuation(x) for x in word_tokenize(atext.lower())]\n",
    "    rejoined = ' '.join(clean_version)\n",
    "    tokenized = [x for x in word_tokenize(rejoined.lower()) if not x in stopped]\n",
    "    return tokenized\n",
    "\n",
    "clean_corpus = []\n",
    "clean_fictioncorpus = []\n",
    "for atext in fictioncorpus:\n",
    "    clean_version = clean_text(atext)\n",
    "    if len(clean_version) > 1:\n",
    "        clean_fictioncorpus.append(clean_version)\n",
    "    \n",
    "print(\"The clean_corpus contains \" + str(len(clean_fictioncorpus)) + \" texts.\")\n",
    "print(type(fictioncorpus))\n",
    "# print(clean_fictioncorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary and create the doc-term matrix\n",
    "\n",
    "The math inside ```gensim``` runs quicker if we know, at the outset, how many words we're dealing with, and represent each word as an integer. So the first stage in building a model is to build a dictionary, which stores words as the values of integer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary made.\n",
      "13178 words.\n",
      "1027 documents.\n",
      "Doc-term matrix extracted.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_fictioncorpus)\n",
    "dictionary.filter_extremes(no_below = 4, no_above = 0.11)\n",
    "\n",
    "# The filter_extremes method allows us to remove words from the dictionary.\n",
    "# In this case we remove words that occur in fewer than 4 documents, or more\n",
    "# than 11% of the documents in the corpus. This is, in effect, another\n",
    "# form of stopwording.\n",
    "\n",
    "# If you had a much larger corpus, you might increase no_below to 10 or 20.\n",
    "\n",
    "print('Dictionary made.')\n",
    "print(len(dictionary), \"words.\")\n",
    "print(len(clean_fictioncorpus), \"documents.\")\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_fictioncorpus if len(doc) > 1]\n",
    "print('Doc-term matrix extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bay\n",
      "carthage\n"
     ]
    }
   ],
   "source": [
    "# Just to show you what's in the dictionary.\n",
    "\n",
    "print(dictionary[1069])\n",
    "print(dictionary[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(36, 1), (91, 1), (165, 1), (202, 1), (215, 1), (220, 1), (253, 1), (283, 1), (319, 1), (398, 3), (436, 1), (541, 1), (568, 1), (665, 1), (699, 2), (719, 1), (828, 1), (851, 1), (868, 1), (872, 1), (958, 1), (1017, 1), (1066, 3), (1074, 1), (1144, 1), (1183, 1), (1199, 1), (1248, 1), (1256, 1), (1268, 1), (1457, 1), (1536, 1), (1554, 1), (1572, 2), (1619, 1), (1643, 1), (1674, 1), (1709, 1), (1768, 1), (1856, 1), (1902, 1), (1925, 1), (2130, 1), (2261, 1), (2334, 2), (2425, 1), (2450, 1), (2486, 1), (2491, 1), (2575, 1), (2629, 1), (2708, 2), (2759, 1), (2789, 1), (2809, 1), (2832, 1), (3074, 1), (3079, 1), (3228, 1), (3273, 1), (3316, 1), (3370, 1), (3375, 1), (3389, 1), (3429, 1), (3464, 1), (3523, 1), (3670, 1), (3684, 1), (3691, 1), (3740, 1), (3756, 1), (3802, 1), (3900, 1), (3940, 2), (4068, 1), (4179, 1), (4275, 1), (4288, 1), (4389, 1), (4415, 1), (4518, 1), (4575, 1), (4617, 1), (4678, 1), (4711, 1), (4746, 1), (4762, 1), (4769, 1), (4779, 1), (4845, 1), (4879, 1), (4955, 1), (4984, 1), (5064, 1), (5068, 1), (5180, 1), (5188, 1), (5289, 1), (5326, 1), (5406, 1), (5427, 1), (5500, 2), (5506, 1), (5539, 1), (5627, 1), (5680, 1), (5717, 1), (5813, 1), (5842, 1), (5862, 1), (5930, 3), (5960, 2), (5990, 1), (6174, 2), (6272, 1), (6303, 1), (6374, 1), (6386, 1), (6406, 1), (6432, 3), (6435, 1), (6436, 1), (6539, 1), (6683, 1), (6770, 1), (6810, 2), (6847, 1), (6861, 1), (6881, 1), (7045, 1), (7212, 1), (7246, 1), (7325, 1), (7331, 1), (7344, 1), (7377, 1), (7434, 1), (7514, 3), (7520, 1), (7747, 1), (7808, 1), (7857, 1), (7946, 1), (8071, 1), (8105, 1), (8202, 1), (8314, 1), (8383, 1), (8403, 1), (8429, 1), (8485, 1), (8530, 1), (8555, 1), (8592, 1), (8767, 1), (8819, 1), (8836, 1), (8854, 3), (8860, 2), (8888, 1), (9118, 1), (9211, 1), (9240, 1), (9285, 1), (9467, 2), (9611, 1), (9614, 2), (9669, 1), (9677, 1), (9688, 1), (9734, 10), (9795, 1), (9803, 1), (9855, 2), (9875, 1), (9904, 2), (9964, 1), (10077, 1), (10123, 1), (10300, 1), (10382, 2), (10391, 1), (10429, 1), (10433, 1), (10439, 1), (10487, 1), (10650, 1), (10679, 1), (10711, 1), (10730, 1), (10758, 1), (10773, 1), (10804, 2), (10855, 1), (10916, 3), (11160, 1), (11226, 1), (11269, 1), (11401, 1), (11430, 1), (11553, 2), (11587, 1), (11591, 1), (11628, 1), (11639, 1), (11671, 1), (11694, 1), (11714, 1), (11774, 1), (11855, 1), (11905, 2), (12036, 1), (12037, 2), (12064, 1), (12080, 1), (12118, 2), (12250, 1), (12415, 1), (12423, 1), (12538, 1), (12564, 1), (12566, 1), (12595, 1), (12609, 1), (12645, 2), (12877, 1), (12895, 2), (12922, 1), (12945, 1), (12977, 1), (13006, 1), (13033, 1), (13040, 1), (13153, 1)]\n"
     ]
    }
   ],
   "source": [
    "# And what our corpus looks like now.\n",
    "# Each tuple contains a word ID, and the number of occurrences of that word.\n",
    "\n",
    "print(doc_term_matrix[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually running LDA\n",
    "\n",
    "The first line here creates an LDA-modeling demon.\n",
    "The second line asks the demon to create a model of our corpus.\n",
    "\n",
    "```num_topics``` and ```passes``` are both parameters you may want to fiddle with. Sixteen topics is a pretty small number. In a larger corpus that would be increased. For our medium corpus, you might try 20 or 25. As with clustering, there are strategies that can attempt to optimize the \"right\" number, but this is in reality a matter of judgement.\n",
    "\n",
    "```passes``` sets the number of iterations. More is better, up to a thousand or so. But for a classroom experiment, we probably don't want to go over 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 16, id2word = dictionary, passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dreaming lyrics willie luther sins joys hell freedom pleasures lays\n",
      "1 de st paris banks england interest des robert page x\n",
      "2 poems farewell autumn tomb ray lake lone joys afar wing\n",
      "3 four israel sword ho chief paradise tonight aaron david r\n",
      "4 hell law freedom hate eleanor nation falls southern evil lion\n",
      "5 boat ship captain sail lieutenant th bee deck shop buck\n",
      "6 says jerusalem doom christ july food ann iv awful flesh\n",
      "7 3 2 jesus clay 4 says joan witness lines 5\n",
      "8 modern boys cave general ar st ball says send milly\n",
      "9 letter table suppose ought idea case business manner certainly happened\n",
      "10 jenny ring pine yellow thick crimson laughter mouth lying magic\n",
      "11 wi hae sae nae frae aye ken laird weel wee\n",
      "12 woods mirth tha keith sculpture identical sculptor fountain canto th\n",
      "13 quot wold nay liberty canto duty margaret baby london poet\n",
      "14 emma saxon company dog sal joan « says kitchen added\n",
      "15 burns smith prince william presented palace c portrait published castle\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_topics(topiclist):\n",
    "    for topicnum, topic in topiclist:\n",
    "        cleanwords = []\n",
    "        pieces = topic.split(' + ')\n",
    "        for p in pieces:\n",
    "            numword = p.split('*')\n",
    "            word = numword[1].strip('\"')\n",
    "            cleanwords.append(word)\n",
    "        print(topicnum, ' '.join(cleanwords))\n",
    "\n",
    "pretty_print_topics(ldamodel.print_topics(num_topics=16, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that impressive? Probably not. The value of topic modeling depends heavily on the size of the corpus, and we are deliberately using small corpora to avoid frying your laptops.\n",
    "\n",
    "If it ran quickly enough you might try increasing the number of iterations to 200. See if those topics seem to make more sense. If *that* runs quickly enough, you might try loading the mediumwikicorpus.csv, to see if you get even more interpretable topics. But it will probably take 10-15 minutes to run, at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS\n",
    "I decided to run the topic modeling on the entire poefic dataset by removing the slicing from the code we (mostly Ted) drafted during class. It's interesting to see slightly better topics here, though not the perfect set of more siloed ideas that I think is the researcher's dream when thinking about and running topic modeling. But, nonetheless, we have some interesting outputs. The poetry data seems to have coalesced around a few topics, most evidently 13 and 11, which show signs of the different verbiage of poetry with \"quot\" and \"wold\" and \"nay.\" Topic 11 also seems to have atypical language, I believe (from my years as an English undergraduate student) that resembles poetry in the Scots dialect (or something else British) similar to Robert Burns.\n",
    "\n",
    "We also have other topics that have grouped together around some potential themese, such as topic 5, which seems like a lot of nautical terms, or at least a few heavily nautical terms wrapped up with others that can be read this way. Similarly, topics like 7, 0 and 4 could be biblical or reglious (the words and the numbers hint at this), about more existential texts, or historical fiction, respectively.\n",
    "\n",
    "However, in the end, as I find with all topic modeling, it is only as useful as your research question. In this case, being relatively ignorant to the depth and breadth of the corpus, and without a research question, I don't find the topics incredibly useful. Personally, topic modeling is neat, and I like the functionality showcased, but even when I've had a research question, it seems incredibly shallow analytically. I think with a long list of stop words tailored to a research question, there could be something compelling with topic modeling. Similarly, using topic modeling to help identify types of texts seems pretty within reach (like poetry in this example). But without that, you are left with results that are sculptable--a good trait sometimes, but not so compelling to use as evidence of any in-depth analysis, in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other things you can do\n",
    "\n",
    "One of the nice things about the gensim module is that it allows you to update an existing model; you can even add documents to the corpus and update the model.\n",
    "\n",
    "In addition to getting the top words for a given topic (topic distribution across terms), you can get the distribution of a document across topics, or the distribution of a word across topics. For more on these options, see [the documentation.](https://radimrehurek.com/gensim/models/ldamodel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.update(doc_term_matrix, iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.98600745228817488)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.023670363990880825)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
