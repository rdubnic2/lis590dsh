{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Web scraping, and other forms of data \"munging.\"\n",
    "\n",
    "There are lots of ways to get data off the web. Ideally, you find a complete dataset already prepared that meets your needs. Click and download.\n",
    "\n",
    "A little less ideally, you find an API that can be queried, and that returns structured data easy to interpret. We've already been doing this with the Google Maps API. Here, you often start with a list of queries for the API.\n",
    "\n",
    "But in some cases, you're going to need to get data from the open web, where it's formatted more for beautiful display than for ease of analysis.\n",
    "\n",
    "The obvious challenge here (opening a URL and getting data from the web into Python) is actually very easy to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<head>\n",
      "<title>A very simple webpage</title>\n",
      "<basefont size=4>\n",
      "</head>\n",
      "\n",
      "<body bgcolor=FFFFFF>\n",
      "\n",
      "<h1>A very simple webpage. This is an \"h1\" level header.</h1>\n",
      "\n",
      "<h2>This is a level h2 header.</h2>\n",
      "\n",
      "<h6>This is a level h6 header.  Pretty small!</h6>\n",
      "\n",
      "<p>This is a standard paragraph.</p>\n",
      "\n",
      "<p align=center>Now I've aligned it in the center of the screen.</p>\n",
      "\n",
      "<p align=right>Now aligned to the right</p>\n",
      "\n",
      "<p><b>Bold text</b></p>\n",
      "\n",
      "<p><strong>Strongly emphasized text</strong>  Can you tell the difference vs. bold?</p>\n",
      "\n",
      "<p><i>Italics</i></p>\n",
      "\n",
      "<p><em>Emphasized text</em>  Just like Italics!</p>\n",
      "\n",
      "<p>Here is a pretty picture: <img src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\n",
      "\n",
      "<p>Same thing, aligned differently to the paragraph: <img align=top src=example/prettypicture.jpg alt=\"Pretty Picture\"></p>\n",
      "\n",
      "<hr>\n",
      "\n",
      "<h2>How about a nice ordered list!</h2>\n",
      "<ol>\n",
      "  <li>This little piggy went to market\n",
      "  <li>This little piggy went to SB228 class\n",
      "  <li>This little piggy went to an expensive restaurant in Downtown Palo Alto\n",
      "  <li>This little piggy ate too much at Indian Buffet.\n",
      "  <li>This little piggy got lost\n",
      "</ol>\n",
      "\n",
      "<h2>Unordered list</h2>\n",
      "<ul>\n",
      "  <li>First element\n",
      "  <li>Second element\n",
      "  <li>Third element\n",
      "</ul>\n",
      "\n",
      "<hr>\n",
      "\n",
      "<h2>Nested Lists!</h2>\n",
      "<ul>\n",
      "  <li>Things to to today:\n",
      "    <ol>\n",
      "      <li>Walk the dog\n",
      "      <li>Feed the cat\n",
      "      <li>Mow the lawn\n",
      "    </ol>\n",
      "  <li>Things to do tomorrow:\n",
      "    <ol>\n",
      "      <li>Lunch with mom\n",
      "      <li>Feed the hamster\n",
      "      <li>Clean kitchen\n",
      "    </ol>\n",
      "</ul>\n",
      "\n",
      "<p>And finally, how about some <a href=http://www.yahoo.com/>Links?</a></p>\n",
      "\n",
      "<p>Or let's just link to <a href=../../index.html>another page on this server</a></p>\n",
      "\n",
      "<p>Remember, you can view the HTMl code from this or any other page by using the \"View Page Source\" command of your browser.</p>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "test_url = \"http://csb.stanford.edu/class/public/pages/sykes_webdesign/05_simple.html\"\n",
    "page = requests.get(test_url)\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was incredibly easy. You can open [http://csb.stanford.edu/class/public/pages/sykes_webdesign/05_simple.html](http://csb.stanford.edu/class/public/pages/sykes_webdesign/05_simple.html) to see what this looks like in a browser. Then select \"Show Page Source\" (or the equivalent in your browser).\n",
    "\n",
    "### The hard parts\n",
    "\n",
    "The hard parts of web scraping are 1) finding the right URL in the first place and 2) extracting what you need from the hierarchical tangle of html tags.\n",
    "\n",
    "1) I don't have a general solution to the first problem. You'll need to exercise some cleverness. Sometimes you can look at the ways URLs are formed and infer how to generate other valid URLs. For instance, if you search \"Stranger Things\" on IMDb, you get this URL\n",
    "\n",
    "[http://www.imdb.com/find?ref_=nv_sr_fn&q=stranger+things&s=all](http://www.imdb.com/find?ref_=nv_sr_fn&q=stranger+things&s=all)\n",
    "\n",
    "It's not too hard to see what you would have to replace there to get the search page for *Star Wars.*\n",
    "\n",
    "Sometimes you need to find URLs by extracting links from other web pages. Which brings us to problem two.\n",
    "\n",
    "2) To get what you need from a web page, you can use an html-parsing module like Beautiful Soup. For instance, let's get the level h6 header from the Very Simple Web Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h6>This is a level h6 header.  Pretty small!</h6>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "soup.find('h6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we want to extract the text, it's as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a level h6 header.  Pretty small!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h6').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's simple enough! Things get a little harder if we want to get the link to \"another page.\" We could try ```.find('a')```, but that would give us the link to Yahoo instead, because it will return the *first* example of a specified tag. We could use ```.find_all()```, but that will return *all* the examples.\n",
    "\n",
    "To find a section of html that includes a given chunk of text, we have to use something called a regular expression, which matches a given pattern. A lot can be said about regular expressions, but the special characters used to match patterns are not easy to memorize; in practice when I need a complex one I go [to Regex 101](https://regex101.com), and play around. In this case, we don't have to. It's dead easy, because we just want to literally match a particular chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"../../index.html\">another page on this server</a>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile('another')\n",
    "soup.find('a', text = regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's lovely, but we actually want the *link*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../index.html'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmlsection = soup.find('a', text = regex)\n",
    "htmlsection.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A real problem.\n",
    "\n",
    "Say we want to scrape IMDb data for a series of movies. First, we're going to need to find the right pages. That's not easy. Here, for instance, is the URL for \"Casablanca\":\n",
    "\n",
    "    http://www.imdb.com/title/tt0034583/?ref_=fn_al_tt_1\n",
    "\n",
    "That's not going to be easy to infer from the title. We're going to have to do what we actually do as human beings â€” use the search function. As noted above, the URLs for searches are pretty transparent, e.g.,\n",
    "\n",
    "    http://www.imdb.com/find?ref_=nv_sr_fn&q=stranger+things&s=all\n",
    "\n",
    "So we can build a function that takes a movie title and returns the IMDb page. While we're doing that, we'll implement some very basic error handling. The internet is a messy place and tends to return errors for no clear reason, so patience is sometimes needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trythreetimes(url):\n",
    "    ''' Sometimes when you initially attempt to load a page, you get an error:\n",
    "    a page with status 404 or 503, etc.\n",
    "    \n",
    "    There are sophisticated ways to handle that, but let's try a simple way:\n",
    "    Keep trying until it works. We'll stop after three tries, so as not to hang\n",
    "    up in an infinite loop.\n",
    "    '''\n",
    "    found = False\n",
    "    tries = 0\n",
    "    \n",
    "    while not found and tries < 3:\n",
    "\n",
    "        if tries > 0:\n",
    "            time.sleep(1)\n",
    "            # maybe the web needs a brief rest?\n",
    "        \n",
    "        tries += 1\n",
    "        \n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            if page.status_code == 200:\n",
    "                # success!\n",
    "                found = True\n",
    "            # otherwise found will stay False\n",
    "            else:\n",
    "                print(page.status_code)\n",
    "            \n",
    "        except Exception:\n",
    "            # something really went wrong; let's quit\n",
    "            tries = 3\n",
    "            page = ''\n",
    "    \n",
    "    # we'll return the found flag so whoever called this\n",
    "    # function knows whether it worked\n",
    "    return page, found\n",
    "\n",
    "def getsearchpage(atitle):\n",
    "    ''' Takes a title string, breaks it into words, joins the\n",
    "    words with a plus character, and pours it into a url of the\n",
    "    correct form for IMDb. Then triesthreetimes.\n",
    "    '''\n",
    "    \n",
    "    words = atitle.split()\n",
    "    url = \"http://www.imdb.com/find?ref_=nv_sr_fn&q=\" + '+'.join(words) + '&s=all'\n",
    "    page, found = trythreetimes(url)\n",
    "    return page, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page, found = getsearchpage('Casablanca')\n",
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that part is working. We can get the html for a search page! Now, you have the fun job of figuring out how to use Beautiful Soup to find the part of that page that contains the link to the full IMDb page, and then extract the link.\n",
    "\n",
    "Mainly, this requires opening the page source on your browser, and using \"find\" (command-F or control-F) to locate the part of that vast mess that contains the title of the movie. (You may often get title results for a search. Let's assume for now that we're interested in the first one; that will usually be true.) Then you need to figure out a series of Beautiful Soup commands that will return the link for that title. Build this as a function, so we can reuse it further down the page. The function should accept a movie title, and return the IMDb link to the main page for that movie.\n",
    "\n",
    "Here's a little hint involving a feature we didn't try above.\n",
    "\n",
    "If you see an html tag like ```<td class=\"a_specific_class\">```\n",
    "\n",
    "You can get that section of html using Beautiful Soup like so:\n",
    "\n",
    "    soup.find('td', 'a_specific_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/title/tt0034583/?ref_=fn_al_tt_1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for the function link2movie goes here\n",
    "\n",
    "def link2movie(movie_title):\n",
    "    ''' Takes a movie title, gets the URL for the\n",
    "    search page for that movie, and then crawls the\n",
    "    search page to get the link to the movie page\n",
    "    itself.\n",
    "    '''\n",
    "    page, found = getsearchpage(movie_title)\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    enclosing_chunk = soup.find('td', 'result_text')\n",
    "    link_tag = enclosing_chunk.find('a')\n",
    "    pagelink = link_tag.get('href')\n",
    "    return pagelink\n",
    "\n",
    "link2movie('Casablanca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A harder version of web scraping, for homework.\n",
    "\n",
    "Now may be the time to admit that IMDb itself actually makes things easier for us. Because they get a lot of use, they've created [a couple of APIs](http://stackoverflow.com/questions/1966503/does-imdb-provide-an-api/7744369#7744369), and even [a bunch of plain text files as static downloads](http://www.imdb.com/interfaces).\n",
    "\n",
    "So we don't actually need to be artful web crawlers to get data out of IMDb. However, many other sites you'll encounter have not been so generous with data, so web crawling is still a useful skill, and IMDb continues to be a fun example.\n",
    "\n",
    "Suppose we want to get data for a series of movies, including (for each movie), the title, the budget and gross, the genres, and the storyline summary. We also have to be prepared for the possibility that some of this data will not be provided for every movie!\n",
    "\n",
    "First, write a function that extracts those fields from an IMDb page.\n",
    "\n",
    "Then, write a loop that cycles through a list of movie titles in order to create a pandas DataFrame. For each title it should\n",
    "1. retrieve the search page for the movie, and get the link to the title page\n",
    "2. load the title page and extract the fields described above, and then\n",
    "3. add those fields to ever-growing lists that can become the columns of a DataFrame\n",
    "4. create a dictionary where the keys are column names and the values are the lists you created in (3); then say ```pd.DataFrame(your_dictionary)``` to create a DataFrame.\n",
    " \n",
    "Try this on a list of five or six movies. ['Casablanca', 'Jaws', 'Bill & Ted's Excellent Adventure', others of your choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another form of data cleaning.\n",
    "\n",
    "We've learned to import files as pandas DataFrames. In the real world, however, files are not created by pandas, but by primates who do not always structure them neatly as DataFrames.\n",
    "\n",
    "Take a look, for instance, at the \"genres.list.gz\" dataset available from the [IMDB static download page](http://www.imdb.com/interfaces). This is one of the static plain text files I described above as an easier option than web scraping. \n",
    "\n",
    "It *is* easier. But. Um. It's not really a table.\n",
    "\n",
    "Suppose we want to chart the rise and fall of genres (as defined by IMDb) over historical time, from 1930 to the present. Our y axis will be, number of films (or TV movies, whatever) in a genre. The x axis will be date.\n",
    "\n",
    "We can get the data we need from this file. Each film is listed multiple times, once for each genre that has been assigned to it. The tricky part is \"date.\" Instead of breaking that out as a separate column, the authors have left it attached to the title (sometimes between two different versions of the title!)\n",
    "\n",
    "Let's tackle this. We're going to need to step through the file line by line, ignoring all lines until we reach the list of titles. Then, for each line, we need to\n",
    "\n",
    "a. break it in two parts at the tab\n",
    "\n",
    "b. somehow extract a date from the title (help us, [Regex 101](https://regex101.com)!)\n",
    "\n",
    "c. keep a running count of counts-per-year *for each genre*. (This may call for a dictionary where the keys are genres, and the values are Counters or lists).\n",
    "\n",
    "d. produce a pandas dataframe where each genre is a column and each row is a year\n",
    "\n",
    "e. finally, make a visualization for a couple of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/genres.list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3f6524c1cc04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgenrecounters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/genres.list'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstill_intro\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/genres.list'"
     ]
    }
   ],
   "source": [
    "## Have at it!\n",
    "import re\n",
    "\n",
    "still_intro = True\n",
    "stop_ctr = 0\n",
    "\n",
    "genrecounters = dict()\n",
    "\n",
    "with open('../data/genres.list', encoding ='latin-1') as f:\n",
    "    for line in f:\n",
    "        if still_intro:\n",
    "            if line.startswith('\"!Next?\"'):\n",
    "                still_intro = False\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        fields = line.split('\\t')\n",
    "        if len(fields[0]) < 1:\n",
    "            continue\n",
    "            # that line was badly formed for some reason\n",
    "        else:\n",
    "            match = re.search('\\([0-9]{4}\\)', fields[0])\n",
    "            matchedstring = match.group(0)\n",
    "            \n",
    "            try:\n",
    "                stringdate = matchedstring[1:5]\n",
    "                intdate = int(stringdate)\n",
    "            except Exception:\n",
    "                # maybe there's no date in this line?\n",
    "                intdate = -1\n",
    "                print(Exception)\n",
    "            \n",
    "            if intdate > 1930:\n",
    "                print(intdate)\n",
    "                stop_ctr += 1\n",
    "            \n",
    "            if stop_ctr > 10:\n",
    "                break\n",
    "                # because this is just a test\n",
    "                # otherwise we would keep going\n",
    "            \n",
    "            # Here is where you would get the genre from\n",
    "            # fields[0] and test to see whether it's already\n",
    "            # in your dict of genrecounters.\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice all the protective code I added there that involves testing to make sure it's a well-formed line, actually has a date as we expect, etcetera. One of the things about data cleaning is that you encounter lots of \"edge cases,\" aka, weird things you might not expect to happen. In a file with thousands or millions of lines, there may be several of these, and you need to set things up so a single error won't kill your whole job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy matching, deduplication, etc.\n",
    "\n",
    "On the syllabus, I describe an absurdly ambitious list of things to cover today. We can't really practice them all. But just for future reference: fuzzy matching is what you need to do, in the real world, when you're working with data created by primates who might create entries for\n",
    "\n",
    "    Shakespeare, William\n",
    "    Shakspear, William\n",
    "    Shakespeare, W.\n",
    "    Shakespeare, W\n",
    "    Shakespeare, W (1564-1616)\n",
    "\n",
    "and you need to recognize those as the same person. How can you do that? Well, we're not going to have time to cover it completely, but here's a useful lead that can take you where you need to go. Watch this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "match = SequenceMatcher(None, 'Shakspear', 'Shakespeare')\n",
    "print(\"Badly spelled: \", match.ratio())\n",
    "match2 = SequenceMatcher(None, 'Shakespeare, W', 'Shakespeare, W.')\n",
    "print(\"Missing a period: \", match2.ratio())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also try to say a few words about git and rsync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
